<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="TensorFlow,CNN,卷积神经网络," />





  <link rel="alternate" href="/atom.xml" title="FengJian's Blog" type="application/atom+xml" />






<meta name="description" content="前言 这是 上一篇博客 的后续和补充，这次对边缘检测算法的升级优化，起源于一个意外事件，前一个版本是使用 TensorFlow 1.0 部署的， 并且是用 TF-Slim API 编写的代码，最近想使用 TensorFlow 1.7 重新部署一遍，本来以为是一件比较容易的事情，结果实操的时候才发现全是坑，首先遇到的就是废弃 API 的问题，TensorFlow 1.0 里面的某些 API 在 Te">
<meta name="keywords" content="TensorFlow,CNN,卷积神经网络">
<meta property="og:type" content="article">
<meta property="og:title" content="手机端运行卷积神经网络实现文档检测功能(二) -- 从 VGG 到 MobileNetV2 知识梳理">
<meta property="og:url" content="http://fengjian0106.github.io/2018/06/02/Document-Scanning-With-TensorFlow-And-OpenCV-Part-Two/index.html">
<meta property="og:site_name" content="FengJian&#39;s Blog">
<meta property="og:description" content="前言 这是 上一篇博客 的后续和补充，这次对边缘检测算法的升级优化，起源于一个意外事件，前一个版本是使用 TensorFlow 1.0 部署的， 并且是用 TF-Slim API 编写的代码，最近想使用 TensorFlow 1.7 重新部署一遍，本来以为是一件比较容易的事情，结果实操的时候才发现全是坑，首先遇到的就是废弃 API 的问题，TensorFlow 1.0 里面的某些 API 在 Te">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://fengjian0106.github.io/images/same_padding_no_strides_transposed.gif">
<meta property="og:image" content="http://fengjian0106.github.io/images/convolution_operation.png">
<meta property="og:image" content="http://fengjian0106.github.io/images/conv2d.png">
<meta property="og:image" content="http://fengjian0106.github.io/images/one_by_one_conv2d.png">
<meta property="og:image" content="http://fengjian0106.github.io/images/depthwise_conv2d.png">
<meta property="og:image" content="http://fengjian0106.github.io/images/separable_conv2d.png">
<meta property="og:image" content="http://fengjian0106.github.io/images/dilation.gif">
<meta property="og:image" content="http://fengjian0106.github.io/images/no_padding_no_strides_transposed.gif">
<meta property="og:image" content="http://fengjian0106.github.io/images/mobilenet_v1_layer_block.png">
<meta property="og:image" content="http://fengjian0106.github.io/images/mobilenet_v1_body_architecture.png">
<meta property="og:image" content="http://fengjian0106.github.io/images/mobilenet_v2_layer_block.png">
<meta property="og:image" content="http://fengjian0106.github.io/images/mobilenet_v2_body_architecture.png">
<meta property="og:image" content="http://fengjian0106.github.io/images/mobilenet_v2_hed_node_summary_ios.png">
<meta property="og:image" content="http://fengjian0106.github.io/images/mobilenet_v2_hed_node_summary_android_32.png">
<meta property="og:image" content="http://fengjian0106.github.io/images/vgg_hed_node_summary_android_32.png">
<meta property="og:updated_time" content="2018-06-02T11:25:27.324Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="手机端运行卷积神经网络实现文档检测功能(二) -- 从 VGG 到 MobileNetV2 知识梳理">
<meta name="twitter:description" content="前言 这是 上一篇博客 的后续和补充，这次对边缘检测算法的升级优化，起源于一个意外事件，前一个版本是使用 TensorFlow 1.0 部署的， 并且是用 TF-Slim API 编写的代码，最近想使用 TensorFlow 1.7 重新部署一遍，本来以为是一件比较容易的事情，结果实操的时候才发现全是坑，首先遇到的就是废弃 API 的问题，TensorFlow 1.0 里面的某些 API 在 Te">
<meta name="twitter:image" content="http://fengjian0106.github.io/images/same_padding_no_strides_transposed.gif">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://fengjian0106.github.io/2018/06/02/Document-Scanning-With-TensorFlow-And-OpenCV-Part-Two/"/>





  <title>手机端运行卷积神经网络实现文档检测功能(二) -- 从 VGG 到 MobileNetV2 知识梳理 | FengJian's Blog</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?eeb1f7ee5ca721cee0bcad8df6a28d5e";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">FengJian's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">iOS Golang node.js Developer</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://fengjian0106.github.io/2018/06/02/Document-Scanning-With-TensorFlow-And-OpenCV-Part-Two/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="FengJian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FengJian's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">手机端运行卷积神经网络实现文档检测功能(二) -- 从 VGG 到 MobileNetV2 知识梳理</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T18:00:00+08:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/06/02/Document-Scanning-With-TensorFlow-And-OpenCV-Part-Two/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/06/02/Document-Scanning-With-TensorFlow-And-OpenCV-Part-Two/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><ul>
<li>这是 <strong><em><a href="http://fengjian0106.github.io/2017/05/08/Document-Scanning-With-TensorFlow-And-OpenCV/">上一篇博客</a></em></strong> 的后续和补充，这次对边缘检测算法的升级优化，起源于一个意外事件，前一个版本是使用 TensorFlow 1.0 部署的， 并且是用 <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim" target="_blank" rel="noopener">TF-Slim API</a> 编写的代码，最近想使用 TensorFlow 1.7 重新部署一遍，本来以为是一件比较容易的事情，结果实操的时候才发现全是坑，首先遇到的就是废弃 API 的问题，TensorFlow 1.0 里面的某些 API 在 TensorFlow 1.7 里面已经是彻底废弃掉不能使用了，这个问题还好办，修改一下代码就行。后面遇到的一个问题就让我彻底傻眼了，用新的代码加载了旧的模型文件，想 Fine Tuning 一下，结果模型不收敛了，从零开始重新训练也是无法收敛，查了挺长时间也没定位到原因，所以，干脆重写一遍代码。</li>
<li>反正都要重写代码了，那也就可以把最近一年学到的新东西融合进来，就当做是效果验证了。引入这些新的技术后，原始模型其实变化挺大的，而且用到的这些技术，又会牵扯出很多比较通用的基础知识，所以从这个角度来说，这篇文章要记录的重点并不是升级优化(升级后的模型，准确性和前一个版本相比并没有明显的区别，但是模型的体积从 <em>4.4M</em> 减小到了 <em>1.6M</em> ，网络的训练过程也比之前容易了许多)，而是对 <strong><em>多个基础知识点的梳理和总结</em></strong> 。</li>
<li>涉及到的知识点比较多，有工程层面的，也有理论算法层面的，和工程相关的内容会尽量用代码片段来展示，遇到理论知识，只会简单的介绍一下，划出重点，不会做数学层面的推导，同时，会在最后的『参考资料』章节中列出更多的参考内容。</li>
<li>趁这个机会也把代码重新整理了一遍，放在了 github 上，<a href="https://github.com/fengjian0106/hed-tutorial-for-document-scanning" target="_blank" rel="noopener">https://github.com/fengjian0106/hed-tutorial-for-document-scanning</a></li>
</ul>
<h3 id="TensorFlow-Code-Style-For-CNN-Net"><a href="#TensorFlow-Code-Style-For-CNN-Net" class="headerlink" title="TensorFlow Code Style For CNN Net"></a>TensorFlow Code Style For CNN Net</h3><p>之前的那个版本，选用 TF-Slim API 编写代码，就是因为这套 API 是比较优雅的，比如想调用一次最基本的卷积层运算，如果直接使用 <a href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d" target="_blank" rel="noopener">tf.nn.conv2d</a> 的话，代码会是下面这个样子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">input = ...</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'conv1_1'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">	kernel = tf.Variable(tf.truncated_normal([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">128</span>], dtype=tf.float32, stddev=<span class="number">1e-1</span>), name=<span class="string">'weights'</span>)</span><br><span class="line">	conv = tf.nn.conv2d(input, kernel, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">	biases = tf.Variable(tf.constant(<span class="number">0.0</span>, shape=[<span class="number">128</span>], dtype=tf.float32), trainable=<span class="keyword">True</span>, name=<span class="string">'biases'</span>)</span><br><span class="line">	bias = tf.nn.bias_add(conv, biases)</span><br><span class="line">	conv1 = tf.nn.relu(bias, name=scope)</span><br></pre></td></tr></table></figure>
<p>如果用 TF-Slim API 编码的话，则会变成下面这种风格：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input = ...</span><br><span class="line">net = slim.conv2d(input, <span class="number">128</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv1_1'</span>)</span><br></pre></td></tr></table></figure>
<p>因为在各种卷积神经网络结构中，通常都会大量的使用卷积运算，构建很多卷积层，并且使用不同的配置参数，所以很明显，TF-Slim 风格的 API 可以很优雅的简化代码。</p>
<p>但是，在看过图像处理领域的一些论文和各种版本的参考代码之后，发现 TF-Slim 还是有一些局限性的。常规的卷积层操作，用 TF-Slim 是可以简化代码，但是神经网络这个领域发展的速度太快了，经常都会有新的论文发表出来，也就经常会遇到一些新的 layer 结构，TF-Slim 并不是总能很方便的表达出这些 layer，因此需要一种更低层一些、但是更灵活，同时还保持优雅的解决办法。</p>
<p>顺着这个思路，后来发现其实 <a href="https://www.tensorflow.org/api_docs/python/tf/layers" target="_blank" rel="noopener">tf.layers</a> 这个 Module 就可以很好的满足前面提到的这些需求。</p>
<p>另外，这次遇到的在 TensorFlow 1.7 上旧模型不收敛的情况，虽然没有准确定位到原因、没找到解决办法，但是分析了一圈后，其实还是怀疑是因为使用 TF-Slim 而引出的问题，虽然 TF-Slim 简化了卷积层相关的代码，但是完整的代码中还是要使用 TensorFlow 中的其他 API 的，TF-Slim 封装出来的抽象度比较高，除了卷积操作的 API，它还封装了其他的一些 API，但是它的抽象设计和 TensorFlow 是有一种分裂感的，混合在一起编程时会觉得有点奇怪，我这次遇到的问题，也可能就是某些 API 使用的不正确而引起的(TF1.0时运行正常，TF1.7时运行不正常)。而 tf.layers 就不会有这种感觉，tf.layers 的抽象度比 TF-Slim 更低一些，它更像是 TensorFlow 的底层 API 的一个延展，并没有引入新的抽象度，这套 API 用起来就更舒服一些。</p>
<p>比如，升级前的 HED 网络，换用 tf.layers 后，代码是下面这个样子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg_style_hed</span><span class="params">(inputs, batch_size, is_training)</span>:</span></span><br><span class="line">    filter_initializer = tf.contrib.layers.xavier_initializer()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> const.use_kernel_regularizer:</span><br><span class="line">        weights_regularizer = tf.contrib.layers.l2_regularizer(scale=<span class="number">0.0005</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        weights_regularizer = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_vgg_conv2d</span><span class="params">(inputs, filters, kernel_size)</span>:</span></span><br><span class="line">        use_bias = <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">if</span> const.use_batch_norm:</span><br><span class="line">            use_bias = <span class="keyword">False</span></span><br><span class="line">            </span><br><span class="line">        outputs = tf.layers.conv2d(inputs,</span><br><span class="line">                                   filters,</span><br><span class="line">                                   kernel_size, </span><br><span class="line">                                   padding=<span class="string">'same'</span>, </span><br><span class="line">                                   activation=<span class="keyword">None</span>, <span class="comment">## call relu after batch normalization</span></span><br><span class="line">                                   use_bias=use_bias,</span><br><span class="line">                                   kernel_initializer=filter_initializer,</span><br><span class="line">                                   kernel_regularizer=weights_regularizer)</span><br><span class="line">        <span class="keyword">if</span> const.use_batch_norm:</span><br><span class="line">            outputs = tf.layers.batch_normalization(outputs, training=is_training)</span><br><span class="line">        outputs = tf.nn.relu(outputs)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_max_pool2d</span><span class="params">(inputs)</span>:</span></span><br><span class="line">        outputs = tf.layers.max_pooling2d(inputs, </span><br><span class="line">                                          [<span class="number">2</span>, <span class="number">2</span>], </span><br><span class="line">                                          strides=(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">                                          padding=<span class="string">'same'</span>)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_dsn_1x1_conv2d</span><span class="params">(inputs, filters)</span>:</span></span><br><span class="line">        use_bias = <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">if</span> const.use_batch_norm:</span><br><span class="line">            use_bias = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">        kernel_size = [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">        outputs = tf.layers.conv2d(inputs,</span><br><span class="line">                                   filters,</span><br><span class="line">                                   kernel_size, </span><br><span class="line">                                   padding=<span class="string">'same'</span>, </span><br><span class="line">                                   activation=<span class="keyword">None</span>, <span class="comment">## no activation</span></span><br><span class="line">                                   use_bias=use_bias, </span><br><span class="line">                                   kernel_initializer=filter_initializer,</span><br><span class="line">                                   kernel_regularizer=weights_regularizer)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> const.use_batch_norm:</span><br><span class="line">            outputs = tf.layers.batch_normalization(outputs, training=is_training)</span><br><span class="line">        <span class="comment">## no activation</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_output_1x1_conv2d</span><span class="params">(inputs, filters)</span>:</span></span><br><span class="line">        kernel_size = [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">        outputs = tf.layers.conv2d(inputs,</span><br><span class="line">                                   filters,</span><br><span class="line">                                   kernel_size, </span><br><span class="line">                                   padding=<span class="string">'same'</span>, </span><br><span class="line">                                   activation=<span class="keyword">None</span>, <span class="comment">## no activation</span></span><br><span class="line">                                   use_bias=<span class="keyword">True</span>, <span class="comment">## use bias</span></span><br><span class="line">                                   kernel_initializer=filter_initializer,</span><br><span class="line">                                   kernel_regularizer=weights_regularizer)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## no batch normalization</span></span><br><span class="line">        <span class="comment">## no activation</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_dsn_deconv2d_with_upsample_factor</span><span class="params">(inputs, filters, upsample_factor)</span>:</span></span><br><span class="line">        kernel_size = [<span class="number">2</span> * upsample_factor, <span class="number">2</span> * upsample_factor]</span><br><span class="line">        outputs = tf.layers.conv2d_transpose(inputs,</span><br><span class="line">                                             filters, </span><br><span class="line">                                             kernel_size, </span><br><span class="line">                                             strides=(upsample_factor, upsample_factor), </span><br><span class="line">                                             padding=<span class="string">'same'</span>, </span><br><span class="line">                                             activation=<span class="keyword">None</span>, <span class="comment">## no activation</span></span><br><span class="line">                                             use_bias=<span class="keyword">True</span>, <span class="comment">## use bias</span></span><br><span class="line">                                             kernel_initializer=filter_initializer,</span><br><span class="line">                                             kernel_regularizer=weights_regularizer)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## no batch normalization</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="comment"># ref https://github.com/s9xie/hed/blob/master/examples/hed/train_val.prototxt</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'hed'</span>, <span class="string">'hed'</span>, [inputs]):</span><br><span class="line">        end_points = &#123;&#125;</span><br><span class="line">        net = inputs</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'conv1'</span>):</span><br><span class="line">            net = _vgg_conv2d(net, <span class="number">12</span>, [<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">            net = _vgg_conv2d(net, <span class="number">12</span>, [<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">            dsn1 = net</span><br><span class="line">            net = _max_pool2d(net)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'conv2'</span>):</span><br><span class="line">            net = _vgg_conv2d(net, <span class="number">24</span>, [<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">            net = _vgg_conv2d(net, <span class="number">24</span>, [<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">            dsn2 = net</span><br><span class="line">            net = _max_pool2d(net)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'conv3'</span>):</span><br><span class="line">            net = _vgg_conv2d(net, <span class="number">48</span>, [<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">            net = _vgg_conv2d(net, <span class="number">48</span>, [<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">            net = _vgg_conv2d(net, <span class="number">48</span>, [<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">            dsn3 = net</span><br><span class="line">            net = _max_pool2d(net)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'conv4'</span>):</span><br><span class="line">            net = _vgg_conv2d(net, <span class="number">96</span>, [<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">            net = _vgg_conv2d(net, <span class="number">96</span>, [<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">            net = _vgg_conv2d(net, <span class="number">96</span>, [<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">            dsn4 = net</span><br><span class="line">            net = _max_pool2d(net)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'conv5'</span>):</span><br><span class="line">            net = _vgg_conv2d(net, <span class="number">192</span>, [<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">            net = _vgg_conv2d(net, <span class="number">192</span>, [<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">            net = _vgg_conv2d(net, <span class="number">192</span>, [<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">            dsn5 = net</span><br><span class="line">            <span class="comment"># net = _max_pool2d(net) # no need this pool layer</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">## dsn layers</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'dsn1'</span>):</span><br><span class="line">            dsn1 = _dsn_1x1_conv2d(dsn1, <span class="number">1</span>)</span><br><span class="line">            <span class="comment">## no need deconv2d</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'dsn2'</span>):</span><br><span class="line">            dsn2 = _dsn_1x1_conv2d(dsn2, <span class="number">1</span>)</span><br><span class="line">            dsn2 = _dsn_deconv2d_with_upsample_factor(dsn2, <span class="number">1</span>, upsample_factor = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'dsn3'</span>):</span><br><span class="line">            dsn3 = _dsn_1x1_conv2d(dsn3, <span class="number">1</span>)</span><br><span class="line">            dsn3 = _dsn_deconv2d_with_upsample_factor(dsn3, <span class="number">1</span>, upsample_factor = <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'dsn4'</span>):</span><br><span class="line">            dsn4 = _dsn_1x1_conv2d(dsn4, <span class="number">1</span>)</span><br><span class="line">            dsn4 = _dsn_deconv2d_with_upsample_factor(dsn4, <span class="number">1</span>, upsample_factor = <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'dsn5'</span>):</span><br><span class="line">            dsn5 = _dsn_1x1_conv2d(dsn5, <span class="number">1</span>)</span><br><span class="line">            dsn5 = _dsn_deconv2d_with_upsample_factor(dsn5, <span class="number">1</span>, upsample_factor = <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">##dsn fuse</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'dsn_fuse'</span>):</span><br><span class="line">            dsn_fuse = tf.concat([dsn1, dsn2, dsn3, dsn4, dsn5], <span class="number">3</span>)</span><br><span class="line">            dsn_fuse = _output_1x1_conv2d(dsn_fuse, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dsn_fuse, dsn1, dsn2, dsn3, dsn4, dsn5</span><br></pre></td></tr></table></figure>
<p>上面这份代码里面的一些细节，会在后面的章节里详细介绍，并且会逐步的演化成 MobileNetV2 style 的 HED 网络。这里首先看一下代码的整体结构，相当于是套用了下面这种形式的模板：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xx_net</span><span class="params">(inputs, batch_size, is_training)</span>:</span></span><br><span class="line">    filter_initializer = tf.contrib.layers.xavier_initializer()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">layer_for_type1</span><span class="params">(inputs, ...)</span>:</span></span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">layer_for_type2</span><span class="params">(inputs, ...)</span>:</span></span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">layer_for_typeN</span><span class="params">(inputs, ...)</span>:</span></span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   <span class="keyword">with</span> tf.variable_scope(<span class="string">'xx_net'</span>, <span class="string">'xx_net'</span>, [inputs]):</span><br><span class="line">        end_points = &#123;&#125;</span><br><span class="line">        net = inputs</span><br><span class="line"></span><br><span class="line">        net = layer_for_type1(net, ...)</span><br><span class="line">        net = layer_for_type1(net, ...)</span><br><span class="line">        net = layer_for_type2(net, ...)</span><br><span class="line">        ...</span><br><span class="line">        net = layer_for_typeN(net, ...)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> net, end_points</span><br></pre></td></tr></table></figure>
<p>这种风格的代码，前面一部分就是定义实现不同功能的各种 layer，后面部分就是用各种 layer 来组装 net 的主体结构。layer 由嵌套函数定义，方便进行各种自定义的配置或组装，net 主体部分，跟 TF-Slim 的风格其实也是类似的，layer 之间的层级关系简单明了，更容易和论文中的配置表格或结构示意图对应起来。我在实现其他网络结构的时候，都是套用的这种代码结构，基本上都能满足灵活性和简洁性的需求。  </p>
<h3 id="矩阵初始化"><a href="#矩阵初始化" class="headerlink" title="矩阵初始化"></a>矩阵初始化</h3><p>矩阵的初始化方法有很多种，在 TensorFlow 里，常规初始化方法的效果对比可以看这篇文章 <a href="https://github.com/udacity/deep-learning/blob/master/weight-initialization/weight_initialization.ipynb" target="_blank" rel="noopener">Weight Initialization</a>，能使用 <a href="https://www.tensorflow.org/api_docs/python/tf/truncated_normal" target="_blank" rel="noopener">tf.truncated_normal</a> 或 <a href="https://www.tensorflow.org/api_docs/python/tf/truncated_normal_initializer" target="_blank" rel="noopener">tf.truncated_normal_initializer</a> 进行初始化，说明已经对这个问题有所掌握了，随着学习的深入，更推荐使用另外一种初始化方法 <strong><em>Xavier initialization</em></strong> ，使用起来也比较简单：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = tf.get_variable(<span class="string">'W'</span>, shape=[<span class="number">784</span>, <span class="number">256</span>], </span><br><span class="line">                    initializer=tf.contrib.layers.xavier_initializer())</span><br></pre></td></tr></table></figure>
<p>关于 Xavier initialization 的更多内容，请参考本文末尾部分列出的资料。</p>
<h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><p><a href="https://github.com/udacity/deep-learning/blob/master/batch-norm/Batch_Normalization_Lesson.ipynb" target="_blank" rel="noopener">Batch Normalization – Lesson</a> 这篇教程对 Batch Normalization 解释的比较清楚，通俗点描述，普通的 Normalization 是对神经网络的输入数据做归一化处理，把输入数据和输出数据的取值都缩放到一个范围内，通常都是 0.0 ~ 1.0 这个区间，而 Batch Normalization 则是把整体的神经网络结构看成是由很多不同的 layer 组成的，对每个 layer 的输入数据再做一次规范化的操作，因为只能在训练的过程中才能获取到每个 layer 上的 input data，而训练过程又是基于 batch 的，所以叫做 <em>Batch Normalization</em>。Batch Normalization 的具体数学公式，这里不详细描述了，有兴趣的读者请参考末尾部分列出的资料，下面仅从工程层面提出一些建议和要注意的细节点。</p>
<h5 id="Batch-Normalization-的优势很明显，尽量使用"><a href="#Batch-Normalization-的优势很明显，尽量使用" class="headerlink" title="Batch Normalization 的优势很明显，尽量使用"></a>Batch Normalization 的优势很明显，尽量使用</h5><p>Batch Normalization 的优势挺多的，比如可以加快模型收敛的速度、可以使用较高的 learning rates、可以降低权重矩阵初始化的难度、可以提高网络的训练效果等等，总而言之，就是要尽量的使用 Batch Normalization 技术。近几年新发表的很多论文中，也是经常看到 Batch Normalization 的身影。</p>
<p>TensorFlow 提供了相关的 API，在 layer 中添加 Batch Normalization 也就是一行代码的事，不过因为 Batch Normalization 里面有一部分参数也是需要参与反向传播过程进行训练的，所以构造优化器的时候，还要额外添加一些代码把 Batch Normalization 的权重参数也包含进去，类似下面这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_vgg_conv2d</span><span class="params">(inputs, filters, kernel_size)</span>:</span></span><br><span class="line">        use_bias = <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">if</span> const.use_batch_norm:</span><br><span class="line">            use_bias = <span class="keyword">False</span></span><br><span class="line">            </span><br><span class="line">        outputs = tf.layers.conv2d(inputs,</span><br><span class="line">                                   filters,</span><br><span class="line">                                   kernel_size, </span><br><span class="line">                                   padding=<span class="string">'same'</span>, </span><br><span class="line">                                   activation=<span class="keyword">None</span>, <span class="comment">## call relu after batch normalization</span></span><br><span class="line">                                   use_bias=use_bias,</span><br><span class="line">                                   kernel_initializer=filter_initializer,</span><br><span class="line">                                   kernel_regularizer=weights_regularizer)</span><br><span class="line">        <span class="keyword">if</span> const.use_batch_norm:</span><br><span class="line">            outputs = tf.layers.batch_normalization(outputs, training=is_training)</span><br><span class="line">        outputs = tf.nn.relu(outputs)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line">        </span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"adam_vars"</span>):</span><br><span class="line">        <span class="keyword">if</span> const.use_batch_norm == <span class="keyword">True</span>:</span><br><span class="line">            <span class="keyword">with</span> tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):</span><br><span class="line">                train_step = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate).minimize(cost)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            train_step = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate).minimize(cost)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h5 id="不需要使用-bias"><a href="#不需要使用-bias" class="headerlink" title="不需要使用 bias"></a>不需要使用 bias</h5><p>从前面的代码片段可以看到，用了 Batch Normalization 后，就不再需要添加 bias 偏移向量了，<a href="https://stackoverflow.com/questions/46256747/can-not-use-both-bias-and-batch-normalization-in-convolution-layers" target="_blank" rel="noopener">Can not use both bias and batch normalization in convolution layers
</a> 这里有解释原因。</p>
<h5 id="在什么位置添加-Batch-Normalization"><a href="#在什么位置添加-Batch-Normalization" class="headerlink" title="在什么位置添加 Batch Normalization"></a>在什么位置添加 Batch Normalization</h5><p>前面有一个典型的代码片段：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_vgg_conv2d</span><span class="params">(inputs, filters, kernel_size)</span>:</span></span><br><span class="line">        outputs = tf.layers.conv2d(inputs,</span><br><span class="line">                                   filters,</span><br><span class="line">                                   kernel_size, </span><br><span class="line">                                   padding=<span class="string">'same'</span>, </span><br><span class="line">                                   activation=<span class="keyword">None</span>, <span class="comment">## call relu after batch normalization</span></span><br><span class="line">                                   use_bias= <span class="keyword">False</span>,</span><br><span class="line">                                   kernel_initializer=filter_initializer,</span><br><span class="line">                                   kernel_regularizer=weights_regularizer)</span><br><span class="line"></span><br><span class="line">        outputs = tf.layers.batch_normalization(outputs, training=is_training)</span><br><span class="line">        outputs = tf.nn.relu(outputs)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<p>这里容易遇到一个陷进，我之前就掉进去过。在看其他代码和资料的时候，也经常看到 convolution + batch_normalization + relu 这种顺序的代码调用，如果理解的不透彻，很有可能会错误的认为在每一个 convolution layer 的后面都应该添加一个 tf.layers.batch_normalization 调用，但是实际上，如果当前 layer 已经是网络结构中最后的 layer 或者已经属于 output layer 了，其实是不应该再使用 Batch Normalization 的。按照定义，是在 layer 的 <strong><em>input</em></strong> 部分添加 Batch Normalization，而代码里看上去像是在 layer 的 output 上调用了一次 Batch Normalization，这只是为了在代码里让 layer 更容易连接起来，而且，如果是第一层 layer，它的输入就是已经归一化处理过的 input label 数据，这也是不需要 Batch Normalization 的，到了最后一层 layer 的时候，理论上来说是需要 Batch Normalization 的，只不过对应到代码上，最后这层 layer 的 Batch Normalization 是添加在倒数第二层 layer 的输出结果上的。所以，在前面 HED 的代码里，_dsn_deconv2d_with_upsample_factor 和 _output_1x1_conv2d 这两种 layer 的封装函数里都是没有 Batch Normalization 的。  </p>
<p>另外，之前展示的代码都是把 batch_normalization 放在了 relu 激活函数的前面，网上的很多代码也是这样写的，其实把 batch_normalization 放在非线性函数的后面也是可以的，而且整体的准确率可能还会有一点点提升，<a href="https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md#bn----before-or-after-relu" target="_blank" rel="noopener">BN – before or after ReLU?</a> 这里有一个简单的数据对比，可以参考。总之，batch_normalization 和激活函数的先后顺序，是可以灵活选择的。</p>
<h5 id="是否还需要使用-Regularizer"><a href="#是否还需要使用-Regularizer" class="headerlink" title="是否还需要使用 Regularizer"></a>是否还需要使用 Regularizer</h5><p>这也是一个容易混淆的地方，其实 Batch Normalization 和 Regularizer 是完全不一样的东西，Batch Normalization 针对的是 layer 的<em>输入数据</em>，而 Regularizer 针对的是 layer 里面的<em>权重矩阵</em>，前者是从数据层面来改善模型的效果，而后者则是通过改善模型自身来提升模型的效果，这两种技术是不冲突的，可以同时使用。</p>
<h3 id="从卷积运算到各种卷积层"><a href="#从卷积运算到各种卷积层" class="headerlink" title="从卷积运算到各种卷积层"></a>从卷积运算到各种卷积层</h3><h5 id="卷积运算"><a href="#卷积运算" class="headerlink" title="卷积运算"></a>卷积运算</h5><p>关于卷积的基本概念，<a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank" rel="noopener">A technical report on convolution arithmetic in the context of deep learning</a> 这里有很直观的动画演示，比如下面这种就是最常见的卷积运算：</p>
<div align="center"><br><img src="/images/same_padding_no_strides_transposed.gif" alt="same padding no strides transposed"><br></div>

<p>其他的学习资料里，通常也是基于一个普通的二维矩阵来描述卷积的运算规则，上图这个例子，就是在一个 shape 为 (height, width) 的矩阵上，使用一个 (3, 3) 的卷积核，然后得到一个 shape 同样为 (height, width) 的矩阵。  </p>
<p>但是在神经网络领域里面，<strong><em>卷积层</em></strong> 的运算规则其实是比上面这种单纯的 <strong><em>卷积运算</em></strong> 稍微更复杂一些的。在神经网络里面，通常会使用一个 shape 为 (batch_size, height, width, channels) 的 Tensor 来表示图像，比如一个 RGBA 的图像，channels 就是 4，经过某种卷积层的运算后，得到一个新的 Tensor，这个新的 Tensor 的 channels 通常又会变成另外一个数值，可见，这个 channel 也是有一定的映射规则的，标准的卷积运算和 channel 结合起来，才构成了神经网络里面的卷积层。</p>
<p>在介绍具体的卷积层之前，先使用下面这种简单的示意图来表示一个卷积运算：</p>
<div align="center"><br><img src="/images/convolution_operation.png" alt="convolution operation"><br></div>

<p>顺着示意图中箭头的方向，左侧是输入矩阵，中间是卷积核，右侧是输出矩阵。</p>
<h5 id="标准卷积层"><a href="#标准卷积层" class="headerlink" title="标准卷积层"></a>标准卷积层</h5><p>TensorFlow 框架里的标准卷积层的定义如下：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.conv2d(  </span><br><span class="line">    input,  </span><br><span class="line">    filter,</span><br><span class="line">    strides,</span><br><span class="line">    padding,</span><br><span class="line">    use_cudnn_on_gpu=<span class="keyword">True</span>,</span><br><span class="line">    data_format=<span class="string">'NHWC'</span>,</span><br><span class="line">    dilations=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>因为这里主要是为了讨论 channel 的映射规则，所以假设采用 ‘SAME’ padding，并且 strides 设置为 1，这样的话，输入的 Tensor 和 输出的 Tensor 中，height 和 width 都是相同的值，输入的 Tensor 的 shape 是 (batch_size, height, width, in_channels)，如果期望的输出 Tensor 的 shape 是 (batch_size, height, width, out_channels)，则作为 filter 的 Tensor 的 shape 应该设置成 (filter_height, filter_width, in_channels, out_channels)，其中的 filter_height 和 filter_width 就对应卷积核的 size，这个函数内部的完整计算过程，可以用下面这个示意图来表示：  </p>
<div align="center"><br><img src="/images/conv2d.png" alt="conv2d"><br></div>

<p>图中的 in_channels 等于 2，out_channels 等于 5，总共有 in_channels*out_channels = 10 个卷积核(同时还有 5 次矩阵加法操作)，仔细看一下这个示意图就会意识到，每一个输出的矩阵都是由两个输入矩阵共同计算出来的，也就是说不同的输入 channel 会一起影响到每一个输出 channel，通道之间是有关联的。</p>
<h5 id="One-By-One-卷积层"><a href="#One-By-One-卷积层" class="headerlink" title="One By One 卷积层"></a>One By One 卷积层</h5><p>这种网络结构和前面介绍的标准卷积层其实是一样的，只不过 filter 的 shape 是 (1, 1, in_channels, out_channels)，也就是说每一个卷积核都只是一个标量值，而非矩阵。表面上看这种结构有点违反『套路』，因为卷积的目的就是要利用周围像素的 <em>加权和</em> 来替代原始位置上的单个像素，或者说卷积每次关注的是一个区域的像素，而非只关注单个像素。</p>
<p>那 1x1 convolution 的目的是什么呢？前面已经提到了，神经网络里面的卷积层，既有卷积运算，也有 channel 之间的运算，所以 1x1 convolution 的重点就在于让不同的 channel 再结合一遍。类似的，也可以用一个简单的示意图表示这种网络结构：</p>
<div align="center"><br><img src="/images/one_by_one_conv2d.png" alt="one_by_one_conv2d"><br></div>

<p>1x1 convolution 的效果，相当于对输入矩阵做了一个简单的标量乘法，它的参数量和计算量都比标准的卷积层少了很多。前面 HED 代码里的 _output_1x1_conv2d 就是一个 1x1 convolution，在后面的讨论中也会遇到多个例子。</p>
<h5 id="Depthwise-卷积层"><a href="#Depthwise-卷积层" class="headerlink" title="Depthwise 卷积层"></a>Depthwise 卷积层</h5><p>标准卷积层运算，不同的输入 channel 会共同参与计算每一个输出 channel，还有另外一种名为 depthwise convolution 的卷积层运算，channel 之间是完全独立的，TensorFlow 里面的定义如下：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.depthwise_conv2d(</span><br><span class="line">    input,</span><br><span class="line">    filter,</span><br><span class="line">    strides,</span><br><span class="line">    padding,</span><br><span class="line">    rate=<span class="keyword">None</span>,</span><br><span class="line">    name=<span class="keyword">None</span>,</span><br><span class="line">    data_format=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>类似的，假设采用 ‘SAME’ padding，并且 strides 设置为 1，最后的三个参数使用默认值，这样的话，输入 Tensor 和 输出 Tensor 的 height 和 width 就会是相同的值，输入的 Tensor 的 shape 是 (batch_size, height, width, in_channels)，filter Tensor 的 shape 是 (filter_height, filter_width, in_channels, channel_multiplier)，则得到的输出 Tensor 的 shape 是 (batch_size, height, width, in_channels * channel_multiplier)，这个函数内部的完整计算过程，可以用下面这个示意图来表示： </p>
<div align="center"><br><img src="/images/depthwise_conv2d.png" alt="depthwise_conv2d"><br></div>

<p>可以看到，输出 Tensor 的 channels 不能是任意值，只能是 in_channels 的整数倍，这也就是参数 channel_multiplier 的含义。</p>
<h5 id="Separable-卷积层"><a href="#Separable-卷积层" class="headerlink" title="Separable 卷积层"></a>Separable 卷积层</h5><p>depthwise convolution 中，channel 之间是完全不会产生互相影响的，这可能也意味着这种方式的模型的复杂度是不够的，所以在实际使用的过程中，separable convolution 是一个更合适的选择，对应的 TensorFlow API 如下：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.separable_conv2d(</span><br><span class="line">    input,</span><br><span class="line">    depthwise_filter,</span><br><span class="line">    pointwise_filter,</span><br><span class="line">    strides,</span><br><span class="line">    padding,</span><br><span class="line">    rate=<span class="keyword">None</span>,</span><br><span class="line">    name=<span class="keyword">None</span>,</span><br><span class="line">    data_format=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>同样的，采用 ‘SAME’ padding，并且 strides 设置为 1，最后的三个参数使用默认值，这样的话，输入 Tensor 和 输出 Tensor 的 height 和 width 就会是相同的值。这个 API 的内部首先执行了一次 depthwise convolution，然后执行了一次 1x1 convolution(pointwise convolution)，所以 depthwise_filter 的 shape 应该设置为 (filter_height, filter_width, in_channels, channel_multiplier)，pointwise_filter 的 shape 应该设置为 (1, 1, channel_multiplier * in_channels, out_channels)，示意图如下：</p>
<div align="center"><br><img src="/images/separable_conv2d.png" alt="separable_conv2d"><br></div>

<p>在使用相同的 in_channels 和 out_channels 参数时，tf.nn.separable_conv2d 的运算量会比 tf.nn.conv2d 更小。</p>
<h3 id="Dilated-Convolutions-Atrous-Convolution-扩张卷积-空洞卷积"><a href="#Dilated-Convolutions-Atrous-Convolution-扩张卷积-空洞卷积" class="headerlink" title="Dilated Convolutions / Atrous Convolution / 扩张卷积 / 空洞卷积"></a>Dilated Convolutions / Atrous Convolution / 扩张卷积 / 空洞卷积</h3><p>前面看到的几种不同的卷积层函数里，可能会有一个参数 rate，如果设置了 rate 并且 rate &gt; 1，则内部执行了另外一种名为 Dilated Convolutions 的卷积运算操作，这种卷积运算的动画示意图如下：</p>
<div align="center"><br><img src="/images/dilation.gif" alt="dilation"><br></div>

<p>在做边缘检测任务的时候，并没有用到 Dilated Convolutions，但是这种卷积操作也是很常用的，比如在 <a href="http://hellodfan.com/2018/03/11/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E8%AE%BA%E6%96%87-DeepLabv3+/" target="_blank" rel="noopener">DeepLab</a> 网络结构的各个版本中，它都是一个很重要的组件，考虑到这篇文章里已经汇总了多种不同的常用卷积操作，出于完整性的考虑，所以也简单提及一下 Atrous Convolution，有兴趣的同学可以进一步深入了解。</p>
<h3 id="转置卷积-反卷积的初始化"><a href="#转置卷积-反卷积的初始化" class="headerlink" title="转置卷积/反卷积的初始化"></a>转置卷积/反卷积的初始化</h3><p>HED 网络中是会用到转置卷积层的，简单回忆一下，transposed convolution 的动画示意图如下：</p>
<div align="center"><br><img src="/images/no_padding_no_strides_transposed.gif" alt="no_padding_no_strides_transposed"><br></div>

<p>前一篇文章里提到过，当时是使用了双线性放大矩阵(bilinear upsampling kernel)来对反卷积的 kernel 进行的初始化，因为 FCN 要求采用这种初始化方案(HED 的论文中并没有明确的要求使用双线性初始化)。这次重写代码的时候，转置卷积层也统一替换成了 Xavier initialization，仍然能够得到很好的训练效果，同时也严格参照了 HED 的<a href="https://github.com/s9xie/hed/blob/master/examples/hed/train_val.prototxt" target="_blank" rel="noopener">参考代码</a>对转置卷积层的 kernel size 进行设置，具体的参数都在前面的函数 _dsn_deconv2d_with_upsample_factor 里面。</p>
<p>如何初始化 transposed convolution 的卷积核，这个问题其实纠结了很长时间，而且在前一个版本的 HED 的代码中，也尝试过用 tf.truncated_normal 初始化 transposed convolution 的 kernel，当时的确是没有训练出想要的效果，所以有点迷信『双线性初始化』，后来在做 UNet 网络的时候，因为已经接触到 Xavier initialization 方案了，所以也尝试了用 Xavier 对反卷积的 kernel 进行初始化，得到的效果很好，所以才开始慢慢的不再强求于『双线性初始化』。</p>
<p>Google 了很多文章，仍然没有找到关于『双线性初始化』的权威解释，只是找到过一些零星的线索，比如有些模型里，会把 deconvolution 的 kernel 的 learning rate 设置为 0，同时采用双线性插值矩阵对该 kernel 进行初始化，相当于就是通过双线性插值算法对输入矩阵进行上采样(放大)。目前我个人的准则就是，除非论文中有明确的强调要采用某种特殊的初始化方法，否则还是首先使用常规的 Tensor 初始化方案。这篇文章的读者朋友们，如果对这个问题有更清晰的答案，也请指教一下，谢谢~</p>
<p>顺便再举个例子，<a href="https://distill.pub/2016/deconv-checkerboard/" target="_blank" rel="noopener">Deconvolution and Checkerboard Artifacts</a> 这里就是用 resize-convolution 替代了常规的 deconvolution。</p>
<h3 id="从-VGG-到-ResNet、Inception、Xception"><a href="#从-VGG-到-ResNet、Inception、Xception" class="headerlink" title="从 VGG 到 ResNet、Inception、Xception"></a>从 VGG 到 ResNet、Inception、Xception</h3><p>前面着重介绍了几种不同的卷积层运算方式，目的就是为了引出这篇文章 <a href="https://towardsdatascience.com/an-intuitive-guide-to-deep-network-architectures-65fdc477db41" target="_blank" rel="noopener">An Intuitive Guide to Deep Network Architectures</a>。VGG 作为一个经典的分类网络模型，它的结构其实是很简单的，就是标准卷积层串联在一起，如果想进一步提高 VGG 网络的准确率，一个比较直观的想法就是串联更多的标准卷积层(让网络变得更深)、在每一层里增加更多的卷积核，想法看上去是对的，但是实际的效果很不好，因为这种方式增加了大量的参数，训练起来自然就更难，而且网络的深度加深后，还会引起一个 <strong><em><a href="https://www.zhihu.com/question/49812013" target="_blank" rel="noopener">梯度消失</a></em></strong> 的问题，所以简单粗暴并不总是有效的，需要想其他的办法。前面给出链接的这篇文章里介绍的三个重要网络结构，ResNet、Inception 和 Xception，就是为了解决这些问题而发展起来的，这三种网络模型使用的 <strong><em>层结构</em></strong>，已经成为了卷积神经网络领域里面的基础技术手段。</p>
<p>关于 ResNet、Inception、Xception 的详细内容，刚才提到的这篇文章就是一个很好的总结，网上也有一份整理过的中文翻译 <a href="https://www.jiqizhixin.com/articles/2017-08-19-4" target="_blank" rel="noopener">无需数学背景，读懂 ResNet、Inception 和 Xception 三大变革性架构</a>，在文末的参考资料里面还会列出几篇很棒的文章或代码。</p>
<p>如果是我自己对这三种网络结构做一个简单的总结，我觉得主要是下面几点：  </p>
<ul>
<li>ResNet(残差网络) 使得训练更深的网络成为一种可能，既然很深的映射关系 Y = F(X) 不容易训练，那就改成训练 Y = F(X) + X，梯度消失问题就不再是一个障碍。</li>
<li>Inception 架构通过增加每一层网络的宽度(使用不同 size 的卷积核，按照卷积核的大小进行分组)来提高网络的准确性，同时为了控制整体的运算量，借助 1x1 convolution 先对每一层的输入 Tensor 进行一个降维操作，减少 input channel 的数量，然后再进入每一个分组，用不同大小的卷积核进行计算。</li>
<li>残差架构可以和分组策略结合，比如 Inception-ResNet 网络结构。</li>
<li>Inception 里面分组的概念使用到极致，就是让每一个通道成为一个独立的分组，在每个 channel 上先分别进行标准的卷积运算，然后再利用 1x1 convolution 得到最终的输出 channel，就其实就是 separable convolution。</li>
</ul>
<h3 id="从-MobileNet-V1-到-MobileNet-V2"><a href="#从-MobileNet-V1-到-MobileNet-V2" class="headerlink" title="从 MobileNet V1 到 MobileNet V2"></a>从 MobileNet V1 到 MobileNet V2</h3><p>ResNet、Inception、Xception 追求的目标，就是在达到更高的准确率的前提下，尽量在模型大小、模型运算速度、模型训练速度这几个指标之间找一个平衡点，如果在准确性上允许一定的损失，但是追求更小的模型和更快的速度，这就直接催生了 MobileNet 或类似的以手机端或嵌入式端为运行环境的网络结构的出现。</p>
<p><a href="https://arxiv.org/pdf/1704.04861.pdf" target="_blank" rel="noopener">MobileNet V1</a> 和 <a href="https://arxiv.org/pdf/1801.04381.pdf" target="_blank" rel="noopener">MobileNet V2</a> 都是基于 Depthwise Separable Convolution 构建的卷积层(类似 Xception，但是并不是和 Xception 使用的 Separable Convolution 完全一致)，这是它满足体积小、速度快的一个关键因素，另外就是精心设计和试验调优出来的层结构，下面就对照论文给出两个版本的代码实现。</p>
<h5 id="MobileNet-V1"><a href="#MobileNet-V1" class="headerlink" title="MobileNet V1"></a>MobileNet V1</h5><p>MobileNet V1 的整体结构其实并没有特别复杂的地方，和 VGG 类似，层和层之间就是普通的串联型的结构，有区别的地方主要在于 layer 的内部，如下图所示：</p>
<div align="center"><br><img src="/images/mobilenet_v1_layer_block.png" alt="mobilenet_v1_layer_block"><br></div>

<p>这个图中没有用箭头表示数据的传递方向，但是只要对卷积神经网络有初步的经验，就能看出来数据是从上往下传递的，左图是标准的卷积层操作，类似于前面 HED 网络中 _vgg_conv2d 函数的结构(回想一下前面说过的 Batch Normalization 和 relu 先后顺序的话题，虽然 Batch Normalization 可以放到激活函数的后面，但是很多论文里面都还是习惯性的放在激活函数的前面，所以这里的代码也会严格的遵照论文中的方式)，右侧的图相当于 separable convolution，但是在中间是有两次 Batch Normalization 的。</p>
<p>论文中用一张如下的表格来描述了整体结构：</p>
<div align="center"><br><img src="/images/mobilenet_v1_body_architecture.png" alt="mobilenet_v1_body_architecture"><br></div>

<p>下面是一份简单的代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mobilenet_v1</span><span class="params">(inputs, alpha, is_training)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> alpha <span class="keyword">not</span> <span class="keyword">in</span> [<span class="number">0.25</span>, <span class="number">0.50</span>, <span class="number">0.75</span>, <span class="number">1.0</span>]:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'alpha must be one of'</span></span><br><span class="line">                         <span class="string">'`0.25`, `0.50`, `0.75` or `1.0` only.'</span>)</span><br><span class="line"></span><br><span class="line">    filter_initializer = tf.contrib.layers.xavier_initializer()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_conv2d</span><span class="params">(inputs, filters, kernel_size, stride, scope=<span class="string">''</span>)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            outputs = tf.layers.conv2d(inputs,</span><br><span class="line">                                    filters, </span><br><span class="line">                                    kernel_size, </span><br><span class="line">                                    strides=(stride, stride),</span><br><span class="line">                                    padding=<span class="string">'same'</span>, </span><br><span class="line">                                    activation=<span class="keyword">None</span>,</span><br><span class="line">                                    use_bias=<span class="keyword">False</span>, </span><br><span class="line">                                    kernel_initializer=filter_initializer)</span><br><span class="line"></span><br><span class="line">            outputs = tf.layers.batch_normalization(outputs, training=is_training)</span><br><span class="line">            outputs = tf.nn.relu(outputs)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_mobilenet_v1_conv2d</span><span class="params">(inputs, </span></span></span><br><span class="line"><span class="function"><span class="params">                          pointwise_conv_filters, </span></span></span><br><span class="line"><span class="function"><span class="params">                          depthwise_conv_kernel_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                          stride, # stride is just for depthwise convolution</span></span></span><br><span class="line"><span class="function"><span class="params">                          scope=<span class="string">''</span>)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'depthwise_conv'</span>):</span><br><span class="line">                <span class="string">'''</span></span><br><span class="line"><span class="string">                tf.layers Module 里面有一个 tf.layers.separable_conv2d 函数，</span></span><br><span class="line"><span class="string">                但是它的内部调用流程是 depthwise convolution --&gt; pointwise convolution --&gt; activation func，</span></span><br><span class="line"><span class="string">                而 MobileNet V1 风格的卷积层的内部调用流程应该是</span></span><br><span class="line"><span class="string">                depthwise conv --&gt; batch norm --&gt; relu --&gt; pointwise conv --&gt; batch norm --&gt; relu，</span></span><br><span class="line"><span class="string">                所以需要用其他的手段组装出想要的调用流程，</span></span><br><span class="line"><span class="string">                一种办法是使用 tf.nn.depthwise_conv2d，但是这个 API 比较底层，代码写起来很笨重。</span></span><br><span class="line"><span class="string">                后来找到了另外一种可行的办法，借助 tf.contrib.layers.separable_conv2d 函数，</span></span><br><span class="line"><span class="string">                tf.contrib.layers.separable_conv2d 的第二个参数 num_outputs 如果设置为 None，</span></span><br><span class="line"><span class="string">                则只会调用内部的 depthwise conv2d 部分，而不执行 pointwise conv2d 部分。</span></span><br><span class="line"><span class="string">                这样就可以组装出 MobileNet V1 需要的 layer 结构了。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                TensorFlow 提供了四种 API，都命名为 separable_conv2d，但是又存在各种细微的差别，</span></span><br><span class="line"><span class="string">                有兴趣的读者可以自行阅读相关文档</span></span><br><span class="line"><span class="string">                tf.contrib.layers.separable_conv2d [Aliases tf.contrib.layers.separable_convolution2d]</span></span><br><span class="line"><span class="string">                VS</span></span><br><span class="line"><span class="string">                tf.keras.backend.separable_conv2d</span></span><br><span class="line"><span class="string">                VS</span></span><br><span class="line"><span class="string">                tf.layers.separable_conv2d</span></span><br><span class="line"><span class="string">                VS</span></span><br><span class="line"><span class="string">                tf.nn.separable_conv2d</span></span><br><span class="line"><span class="string">                '''</span></span><br><span class="line">                outputs = tf.contrib.layers.separable_conv2d(</span><br><span class="line">                            inputs,</span><br><span class="line">                            <span class="keyword">None</span>, <span class="comment"># ref https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.py</span></span><br><span class="line">                            depthwise_conv_kernel_size,</span><br><span class="line">                            depth_multiplier=<span class="number">1</span>, <span class="comment"># 按照论文的描述，这里设置成1</span></span><br><span class="line">                            stride=(stride, stride),</span><br><span class="line">                            padding=<span class="string">'SAME'</span>,</span><br><span class="line">                            activation_fn=<span class="keyword">None</span>,</span><br><span class="line">                            weights_initializer=filter_initializer,</span><br><span class="line">                            biases_initializer=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">                outputs = tf.layers.batch_normalization(outputs, training=is_training)</span><br><span class="line">                outputs = tf.nn.relu(outputs)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'pointwise_conv'</span>):</span><br><span class="line">                <span class="comment"># 论文中 alpha 参数的含义，就是在每一层的 pointwise conv 的位置按比例缩小输出 channels 的数量</span></span><br><span class="line">                pointwise_conv_filters = int(pointwise_conv_filters * alpha)</span><br><span class="line">                outputs = tf.layers.conv2d(outputs,</span><br><span class="line">                                        pointwise_conv_filters,</span><br><span class="line">                                        (<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">                                        padding=<span class="string">'same'</span>, </span><br><span class="line">                                        activation=<span class="keyword">None</span>,</span><br><span class="line">                                        use_bias=<span class="keyword">False</span>, </span><br><span class="line">                                        kernel_initializer=filter_initializer)</span><br><span class="line"></span><br><span class="line">                outputs = tf.layers.batch_normalization(outputs, training=is_training)</span><br><span class="line">                outputs = tf.nn.relu(outputs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_avg_pool2d</span><span class="params">(inputs, scope=<span class="string">''</span>)</span>:</span></span><br><span class="line">        inputs_shape = inputs.get_shape().as_list()</span><br><span class="line">        <span class="keyword">assert</span> len(inputs_shape) == <span class="number">4</span></span><br><span class="line"></span><br><span class="line">        pool_height = inputs_shape[<span class="number">1</span>]</span><br><span class="line">        pool_width = inputs_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            outputs = tf.layers.average_pooling2d(inputs,</span><br><span class="line">                                      [pool_height, pool_width],</span><br><span class="line">                                      strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">                                      padding=<span class="string">'valid'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    执行分类任务的网络结构，通常还可以作为实现其他任务的网络结构的 base architecture，</span></span><br><span class="line"><span class="string">    为了方便代码复用，这里只需要实现出卷积层构成的主体部分，</span></span><br><span class="line"><span class="string">    外部调用者根据各自的需求使用这里返回的 output 和 end_points。</span></span><br><span class="line"><span class="string">    比如对于分类任务，按照如下方式使用这个函数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    image_height = 224</span></span><br><span class="line"><span class="string">    image_width = 224</span></span><br><span class="line"><span class="string">    image_channels = 3</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    x = tf.placeholder(tf.float32, [None, image_height, image_width, image_channels])</span></span><br><span class="line"><span class="string">    is_training = tf.placeholder(tf.bool, name='is_training')</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    output, net = mobilenet_v1(x, 1.0, is_training)</span></span><br><span class="line"><span class="string">    print('output shape is: %r' % (output.get_shape().as_list()))</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    output = tf.layers.flatten(output)</span></span><br><span class="line"><span class="string">    output = tf.layers.dense(output,</span></span><br><span class="line"><span class="string">                        units=1024, # 1024 class</span></span><br><span class="line"><span class="string">                        activation=None,</span></span><br><span class="line"><span class="string">                        use_bias=True,</span></span><br><span class="line"><span class="string">                        kernel_initializer=tf.contrib.layers.xavier_initializer())</span></span><br><span class="line"><span class="string">    print('output shape is: %r' % (output.get_shape().as_list()))</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'mobilenet'</span>, <span class="string">'mobilenet'</span>, [inputs]):</span><br><span class="line">        end_points = &#123;&#125;</span><br><span class="line">        net = inputs </span><br><span class="line"></span><br><span class="line">        net = _conv2d(net, <span class="number">32</span>, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">2</span>, scope=<span class="string">'block0'</span>)</span><br><span class="line">        end_points[<span class="string">'block0'</span>] = net</span><br><span class="line">        net = _mobilenet_v1_conv2d(net, <span class="number">64</span>, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">1</span>, scope=<span class="string">'block1'</span>)</span><br><span class="line">        end_points[<span class="string">'block1'</span>] = net</span><br><span class="line"></span><br><span class="line">        net = _mobilenet_v1_conv2d(net, <span class="number">128</span>, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">2</span>, scope=<span class="string">'block2'</span>)</span><br><span class="line">        end_points[<span class="string">'block2'</span>] = net</span><br><span class="line">        net = _mobilenet_v1_conv2d(net, <span class="number">128</span>, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">1</span>, scope=<span class="string">'block3'</span>)</span><br><span class="line">        end_points[<span class="string">'block3'</span>] = net</span><br><span class="line"></span><br><span class="line">        net = _mobilenet_v1_conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">2</span>, scope=<span class="string">'block4'</span>)</span><br><span class="line">        end_points[<span class="string">'block4'</span>] = net</span><br><span class="line">        net = _mobilenet_v1_conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">1</span>, scope=<span class="string">'block5'</span>)</span><br><span class="line">        end_points[<span class="string">'block5'</span>] = net</span><br><span class="line"></span><br><span class="line">        net = _mobilenet_v1_conv2d(net, <span class="number">512</span>, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">2</span>, scope=<span class="string">'block6'</span>)</span><br><span class="line">        end_points[<span class="string">'block6'</span>] = net</span><br><span class="line">        net = _mobilenet_v1_conv2d(net, <span class="number">512</span>, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">1</span>, scope=<span class="string">'block7'</span>)</span><br><span class="line">        end_points[<span class="string">'block7'</span>] = net</span><br><span class="line">        net = _mobilenet_v1_conv2d(net, <span class="number">512</span>, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">1</span>, scope=<span class="string">'block8'</span>)</span><br><span class="line">        end_points[<span class="string">'block8'</span>] = net</span><br><span class="line">        net = _mobilenet_v1_conv2d(net, <span class="number">512</span>, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">1</span>, scope=<span class="string">'block9'</span>)</span><br><span class="line">        end_points[<span class="string">'block9'</span>] = net</span><br><span class="line">        net = _mobilenet_v1_conv2d(net, <span class="number">512</span>, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">1</span>, scope=<span class="string">'block10'</span>)</span><br><span class="line">        end_points[<span class="string">'block10'</span>] = net</span><br><span class="line">        net = _mobilenet_v1_conv2d(net, <span class="number">512</span>, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">1</span>, scope=<span class="string">'block11'</span>)</span><br><span class="line">        end_points[<span class="string">'block11'</span>] = net</span><br><span class="line"></span><br><span class="line">        net = _mobilenet_v1_conv2d(net, <span class="number">1024</span>, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">2</span>, scope=<span class="string">'block12'</span>)</span><br><span class="line">        end_points[<span class="string">'block12'</span>] = net</span><br><span class="line">        net = _mobilenet_v1_conv2d(net, <span class="number">1024</span>, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">1</span>, scope=<span class="string">'block13'</span>)</span><br><span class="line">        end_points[<span class="string">'block13'</span>] = net</span><br><span class="line"></span><br><span class="line">        output = _avg_pool2d(net, scope=<span class="string">'output'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> output, end_points</span><br></pre></td></tr></table></figure>
<h5 id="MobileNet-V2"><a href="#MobileNet-V2" class="headerlink" title="MobileNet V2"></a>MobileNet V2</h5><p>MobileNet V2 的改动就比较大了，首先引入了两种新的 layer 结构，如下图所示：</p>
<div align="center"><br><img src="/images/mobilenet_v2_layer_block.png" alt="mobilenet_v2_layer_block"><br></div>

<p>很明显的一个差异点，就是左边这种层结构引入了残差网络的手段，另外，这两种层结构中，在 depthwise convolution 之前又添加了一个 1x1 convolution 操作，在之前举得几个例子中，1x1 convolution 都是用来降维的，而在 MobileNet V2 里，这个位于 depthwise convolution 之前的 1x1 convolution 其实用来提升维度的，对应论文中 <strong><em>expansion factor</em></strong> 参数的含义，在 depthwise convolution 之后仍然还有一次 1x1 convolution 调用，但是这个 1x1 convolution 并不会跟随一个激活函数，只是一次线性变换，所以这里也不叫做 pointwise convolution，而是对应论文中的 1x1 projection convolution。</p>
<p>网络的整体结构由下面的表格描述：</p>
<div align="center"><br><img src="/images/mobilenet_v2_body_architecture.png" alt="mobilenet_v2_body_architecture"><br></div>

<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mobilenet_v2_func_blocks</span><span class="params">(is_training)</span>:</span></span><br><span class="line">    filter_initializer = tf.contrib.layers.xavier_initializer()</span><br><span class="line">    activation_func = tf.nn.relu6</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(inputs, filters, kernel_size, stride, scope=<span class="string">''</span>)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'conv2d'</span>):</span><br><span class="line">                outputs = tf.layers.conv2d(inputs,</span><br><span class="line">                                        filters, </span><br><span class="line">                                        kernel_size, </span><br><span class="line">                                        strides=(stride, stride),</span><br><span class="line">                                        padding=<span class="string">'same'</span>, </span><br><span class="line">                                        activation=<span class="keyword">None</span>,</span><br><span class="line">                                        use_bias=<span class="keyword">False</span>, </span><br><span class="line">                                        kernel_initializer=filter_initializer)</span><br><span class="line">                                        </span><br><span class="line">                outputs = tf.layers.batch_normalization(outputs, training=is_training)</span><br><span class="line">                outputs = tf.nn.relu(outputs)</span><br><span class="line">            <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_1x1_conv2d</span><span class="params">(inputs, filters, stride)</span>:</span></span><br><span class="line">        kernel_size = [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'1x1_conv2d'</span>):</span><br><span class="line">            outputs = tf.layers.conv2d(inputs,</span><br><span class="line">                                    filters, </span><br><span class="line">                                    kernel_size, </span><br><span class="line">                                    strides=(stride, stride),</span><br><span class="line">                                    padding=<span class="string">'same'</span>, </span><br><span class="line">                                    activation=<span class="keyword">None</span>,</span><br><span class="line">                                    use_bias=<span class="keyword">False</span>, </span><br><span class="line">                                    kernel_initializer=filter_initializer)</span><br><span class="line">            </span><br><span class="line">            outputs = tf.layers.batch_normalization(outputs, training=is_training)</span><br><span class="line">            <span class="comment"># no activation_func</span></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">expansion_conv2d</span><span class="params">(inputs, expansion, stride)</span>:</span></span><br><span class="line">        input_shape = inputs.get_shape().as_list()</span><br><span class="line">        <span class="keyword">assert</span> len(input_shape) == <span class="number">4</span></span><br><span class="line">        filters = input_shape[<span class="number">3</span>] * expansion</span><br><span class="line"></span><br><span class="line">        kernel_size = [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'expansion_1x1_conv2d'</span>):</span><br><span class="line">            outputs = tf.layers.conv2d(inputs,</span><br><span class="line">                                    filters, </span><br><span class="line">                                    kernel_size, </span><br><span class="line">                                    strides=(stride, stride),</span><br><span class="line">                                    padding=<span class="string">'same'</span>, </span><br><span class="line">                                    activation=<span class="keyword">None</span>,</span><br><span class="line">                                    use_bias=<span class="keyword">False</span>, </span><br><span class="line">                                    kernel_initializer=filter_initializer)</span><br><span class="line"></span><br><span class="line">            outputs = tf.layers.batch_normalization(outputs, training=is_training)</span><br><span class="line">            outputs = activation_func(outputs)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">projection_conv2d</span><span class="params">(inputs, filters, stride)</span>:</span></span><br><span class="line">        kernel_size = [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'projection_1x1_conv2d'</span>):</span><br><span class="line">            outputs = tf.layers.conv2d(inputs,</span><br><span class="line">                                    filters, </span><br><span class="line">                                    kernel_size, </span><br><span class="line">                                    strides=(stride, stride),</span><br><span class="line">                                    padding=<span class="string">'same'</span>, </span><br><span class="line">                                    activation=<span class="keyword">None</span>,</span><br><span class="line">                                    use_bias=<span class="keyword">False</span>, </span><br><span class="line">                                    kernel_initializer=filter_initializer)</span><br><span class="line"></span><br><span class="line">            outputs = tf.layers.batch_normalization(outputs, training=is_training)</span><br><span class="line">            <span class="comment"># no activation_func</span></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">depthwise_conv2d</span><span class="params">(inputs, </span></span></span><br><span class="line"><span class="function"><span class="params">                        depthwise_conv_kernel_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                        stride)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'depthwise_conv2d'</span>):</span><br><span class="line">            outputs = tf.contrib.layers.separable_conv2d(</span><br><span class="line">                        inputs,</span><br><span class="line">                        <span class="keyword">None</span>, <span class="comment"># https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.py</span></span><br><span class="line">                        depthwise_conv_kernel_size,</span><br><span class="line">                        depth_multiplier=<span class="number">1</span>,</span><br><span class="line">                        stride=(stride, stride),</span><br><span class="line">                        padding=<span class="string">'SAME'</span>,</span><br><span class="line">                        activation_fn=<span class="keyword">None</span>,</span><br><span class="line">                        weights_initializer=filter_initializer,</span><br><span class="line">                        biases_initializer=<span class="keyword">None</span>) </span><br><span class="line"></span><br><span class="line">            outputs = tf.layers.batch_normalization(outputs, training=is_training)</span><br><span class="line">            outputs = activation_func(outputs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">avg_pool2d</span><span class="params">(inputs, scope=<span class="string">''</span>)</span>:</span></span><br><span class="line">        inputs_shape = inputs.get_shape().as_list()</span><br><span class="line">        <span class="keyword">assert</span> len(inputs_shape) == <span class="number">4</span></span><br><span class="line"></span><br><span class="line">        pool_height = inputs_shape[<span class="number">1</span>]</span><br><span class="line">        pool_width = inputs_shape[<span class="number">2</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            outputs = tf.layers.average_pooling2d(inputs,</span><br><span class="line">                                            [pool_height, pool_width],</span><br><span class="line">                                            strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">                                            padding=<span class="string">'valid'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inverted_residual_block</span><span class="params">(inputs, </span></span></span><br><span class="line"><span class="function"><span class="params">                            filters, </span></span></span><br><span class="line"><span class="function"><span class="params">                            stride, </span></span></span><br><span class="line"><span class="function"><span class="params">                            expansion=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">                            scope=<span class="string">''</span>)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> stride == <span class="number">1</span> <span class="keyword">or</span> stride == <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        depthwise_conv_kernel_size = [<span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line">        pointwise_conv_filters = filters</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            net = inputs</span><br><span class="line">            net = expansion_conv2d(net, expansion, stride=<span class="number">1</span>)</span><br><span class="line">            net = depthwise_conv2d(net, depthwise_conv_kernel_size, stride=stride)</span><br><span class="line">            net = projection_conv2d(net, pointwise_conv_filters, stride=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> stride == <span class="number">1</span>:</span><br><span class="line">                <span class="comment"># 如果 net.get_shape().as_list()[3] != inputs.get_shape().as_list()[3]</span></span><br><span class="line">                <span class="comment"># 借助一个 1x1 的卷积让他们的 channels 相等，然后才能相加</span></span><br><span class="line">                <span class="keyword">if</span> net.get_shape().as_list()[<span class="number">3</span>] != inputs.get_shape().as_list()[<span class="number">3</span>]:</span><br><span class="line">                    inputs = _1x1_conv2d(inputs, net.get_shape().as_list()[<span class="number">3</span>], stride=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                net = net + inputs</span><br><span class="line">                <span class="keyword">return</span> net</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># stride == 2</span></span><br><span class="line">                <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">    func_blocks = &#123;&#125;</span><br><span class="line">    func_blocks[<span class="string">'conv2d'</span>] = conv2d</span><br><span class="line">    func_blocks[<span class="string">'inverted_residual_block'</span>] = inverted_residual_block</span><br><span class="line">    func_blocks[<span class="string">'avg_pool2d'</span>] = avg_pool2d</span><br><span class="line">    func_blocks[<span class="string">'filter_initializer'</span>] = filter_initializer</span><br><span class="line">    func_blocks[<span class="string">'activation_func'</span>] = activation_func</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> func_blocks</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mobilenet_v2</span><span class="params">(inputs, is_training)</span>:</span></span><br><span class="line">    func_blocks = mobilenet_v2_func_blocks(is_training)</span><br><span class="line">    _conv2d = func_blocks[<span class="string">'conv2d'</span>] </span><br><span class="line">    _inverted_residual_block = func_blocks[<span class="string">'inverted_residual_block'</span>]</span><br><span class="line">    _avg_pool2d = func_blocks[<span class="string">'avg_pool2d'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'mobilenet_v2'</span>, <span class="string">'mobilenet_v2'</span>, [inputs]):</span><br><span class="line">        end_points = &#123;&#125;</span><br><span class="line">        net = inputs </span><br><span class="line">    </span><br><span class="line">        net = _conv2d(net, <span class="number">32</span>, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">2</span>, scope=<span class="string">'block0_0'</span>) <span class="comment"># size/2</span></span><br><span class="line">        end_points[<span class="string">'block0'</span>] = net</span><br><span class="line"></span><br><span class="line">        net = _inverted_residual_block(net, <span class="number">16</span>, stride=<span class="number">1</span>, expansion=<span class="number">1</span>, scope=<span class="string">'block1_0'</span>)</span><br><span class="line">        end_points[<span class="string">'block1'</span>] = net</span><br><span class="line"></span><br><span class="line">        net = _inverted_residual_block(net, <span class="number">24</span>, stride=<span class="number">2</span>, scope=<span class="string">'block2_0'</span>) <span class="comment"># size/4</span></span><br><span class="line">        net = _inverted_residual_block(net, <span class="number">24</span>, stride=<span class="number">1</span>, scope=<span class="string">'block2_1'</span>)</span><br><span class="line">        end_points[<span class="string">'block2'</span>] = net</span><br><span class="line"></span><br><span class="line">        net = _inverted_residual_block(net, <span class="number">32</span>, stride=<span class="number">2</span>, scope=<span class="string">'block3_0'</span>) <span class="comment"># size/8</span></span><br><span class="line">        net = _inverted_residual_block(net, <span class="number">32</span>, stride=<span class="number">1</span>, scope=<span class="string">'block3_1'</span>) </span><br><span class="line">        net = _inverted_residual_block(net, <span class="number">32</span>, stride=<span class="number">1</span>, scope=<span class="string">'block3_2'</span>)</span><br><span class="line">        end_points[<span class="string">'block3'</span>] = net</span><br><span class="line"></span><br><span class="line">        net = _inverted_residual_block(net, <span class="number">64</span>, stride=<span class="number">2</span>, scope=<span class="string">'block4_0'</span>) <span class="comment"># size/16</span></span><br><span class="line">        net = _inverted_residual_block(net, <span class="number">64</span>, stride=<span class="number">1</span>, scope=<span class="string">'block4_1'</span>) </span><br><span class="line">        net = _inverted_residual_block(net, <span class="number">64</span>, stride=<span class="number">1</span>, scope=<span class="string">'block4_2'</span>) </span><br><span class="line">        net = _inverted_residual_block(net, <span class="number">64</span>, stride=<span class="number">1</span>, scope=<span class="string">'block4_3'</span>) </span><br><span class="line">        end_points[<span class="string">'block4'</span>] = net</span><br><span class="line"></span><br><span class="line">        net = _inverted_residual_block(net, <span class="number">96</span>, stride=<span class="number">1</span>, scope=<span class="string">'block5_0'</span>) </span><br><span class="line">        net = _inverted_residual_block(net, <span class="number">96</span>, stride=<span class="number">1</span>, scope=<span class="string">'block5_1'</span>)</span><br><span class="line">        net = _inverted_residual_block(net, <span class="number">96</span>, stride=<span class="number">1</span>, scope=<span class="string">'block5_2'</span>)</span><br><span class="line">        end_points[<span class="string">'block5'</span>] = net</span><br><span class="line"></span><br><span class="line">        net = _inverted_residual_block(net, <span class="number">160</span>, stride=<span class="number">2</span>, scope=<span class="string">'block6_0'</span>) <span class="comment"># size/32</span></span><br><span class="line">        net = _inverted_residual_block(net, <span class="number">160</span>, stride=<span class="number">1</span>, scope=<span class="string">'block6_1'</span>) </span><br><span class="line">        net = _inverted_residual_block(net, <span class="number">160</span>, stride=<span class="number">1</span>, scope=<span class="string">'block6_2'</span>) </span><br><span class="line">        end_points[<span class="string">'block6'</span>] = net</span><br><span class="line"></span><br><span class="line">        net = _inverted_residual_block(net, <span class="number">320</span>, stride=<span class="number">1</span>, scope=<span class="string">'block7_0'</span>)</span><br><span class="line">        end_points[<span class="string">'block7'</span>] = net</span><br><span class="line"></span><br><span class="line">        net = _conv2d(net, <span class="number">1280</span>, [<span class="number">1</span>, <span class="number">1</span>], stride=<span class="number">1</span>, scope=<span class="string">'block8_0'</span>) </span><br><span class="line">        end_points[<span class="string">'block8'</span>] = net</span><br><span class="line"></span><br><span class="line">        output = _avg_pool2d(net, scope=<span class="string">'output'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output, end_points</span><br></pre></td></tr></table></figure>
<h3 id="MobileNet-V2-Style-HED"><a href="#MobileNet-V2-Style-HED" class="headerlink" title="MobileNet V2 Style HED"></a>MobileNet V2 Style HED</h3><p>原始的 HED 使用 VGG 作为基础网络结构来得到 <strong><em>feature maps</em></strong>，参照这种思路，可以把基础网络部分替换为 MobileNet V2，代码如下：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mobilenet_v2_style_hed</span><span class="params">(inputs, batch_size, is_training)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> const.use_kernel_regularizer:</span><br><span class="line">        weights_regularizer = tf.contrib.layers.l2_regularizer(scale=<span class="number">0.0001</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        weights_regularizer = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">####################################################</span></span><br><span class="line">    func_blocks = mobilenet_v2_func_blocks(is_training)</span><br><span class="line">    <span class="comment"># print('============ func_blocks are: %r' % func_blocks)</span></span><br><span class="line">    _conv2d = func_blocks[<span class="string">'conv2d'</span>] </span><br><span class="line">    _inverted_residual_block = func_blocks[<span class="string">'inverted_residual_block'</span>]</span><br><span class="line">    _avg_pool2d = func_blocks[<span class="string">'avg_pool2d'</span>]</span><br><span class="line">    filter_initializer = func_blocks[<span class="string">'filter_initializer'</span>]</span><br><span class="line">    activation_func = func_blocks[<span class="string">'activation_func'</span>]</span><br><span class="line">    <span class="comment">####################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_dsn_1x1_conv2d</span><span class="params">(inputs, filters)</span>:</span></span><br><span class="line">        kernel_size = [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">        outputs = tf.layers.conv2d(inputs,</span><br><span class="line">                                   filters,</span><br><span class="line">                                   kernel_size, </span><br><span class="line">                                   padding=<span class="string">'same'</span>, </span><br><span class="line">                                   activation=<span class="keyword">None</span>, <span class="comment">## no activation</span></span><br><span class="line">                                   use_bias=<span class="keyword">False</span>, </span><br><span class="line">                                   kernel_initializer=filter_initializer,</span><br><span class="line">                                   kernel_regularizer=weights_regularizer)</span><br><span class="line"></span><br><span class="line">        outputs = tf.layers.batch_normalization(outputs, training=is_training)</span><br><span class="line">        <span class="comment">## no activation</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_output_1x1_conv2d</span><span class="params">(inputs, filters)</span>:</span></span><br><span class="line">        kernel_size = [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">        outputs = tf.layers.conv2d(inputs,</span><br><span class="line">                                   filters,</span><br><span class="line">                                   kernel_size, </span><br><span class="line">                                   padding=<span class="string">'same'</span>, </span><br><span class="line">                                   activation=<span class="keyword">None</span>, <span class="comment">## no activation</span></span><br><span class="line">                                   use_bias=<span class="keyword">True</span>, <span class="comment">## use bias</span></span><br><span class="line">                                   kernel_initializer=filter_initializer,</span><br><span class="line">                                   kernel_regularizer=weights_regularizer)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## no batch normalization</span></span><br><span class="line">        <span class="comment">## no activation</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_dsn_deconv2d_with_upsample_factor</span><span class="params">(inputs, filters, upsample_factor)</span>:</span></span><br><span class="line">        <span class="comment">## https://github.com/s9xie/hed/blob/master/examples/hed/train_val.prototxt</span></span><br><span class="line">        <span class="comment">## 从这个原版代码里看，是这样计算 kernel_size 的</span></span><br><span class="line">        kernel_size = [<span class="number">2</span> * upsample_factor, <span class="number">2</span> * upsample_factor]</span><br><span class="line">        outputs = tf.layers.conv2d_transpose(inputs,</span><br><span class="line">                                             filters, </span><br><span class="line">                                             kernel_size, </span><br><span class="line">                                             strides=(upsample_factor, upsample_factor), </span><br><span class="line">                                             padding=<span class="string">'same'</span>, </span><br><span class="line">                                             activation=<span class="keyword">None</span>, <span class="comment">## no activation</span></span><br><span class="line">                                             use_bias=<span class="keyword">True</span>, <span class="comment">## use bias</span></span><br><span class="line">                                             kernel_initializer=filter_initializer,</span><br><span class="line">                                             kernel_regularizer=weights_regularizer)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## 概念上来说，deconv2d 已经是最后的输出 layer 了，只不过最后还有一步 1x1 的 conv2d 把 5 个 deconv2d 的输出再融合到一起</span></span><br><span class="line">        <span class="comment">## 所以不需要再使用 batch normalization 了</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'hed'</span>, <span class="string">'hed'</span>, [inputs]):</span><br><span class="line">        end_points = &#123;&#125;</span><br><span class="line">        net = inputs</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        <span class="comment">## mobilenet v2 as base net</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'mobilenet_v2'</span>):</span><br><span class="line">            <span class="comment"># 标准的 mobilenet v2 里面并没有这两层，</span></span><br><span class="line">            <span class="comment"># 这里是为了得到和 input image 相同 size 的 feature map 而增加的层</span></span><br><span class="line">            net = _conv2d(net, <span class="number">3</span>, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">1</span>, scope=<span class="string">'block0_0'</span>)</span><br><span class="line">            net = _conv2d(net, <span class="number">6</span>, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">1</span>, scope=<span class="string">'block0_1'</span>)</span><br><span class="line"></span><br><span class="line">            dsn1 = net</span><br><span class="line">            net = _conv2d(net, <span class="number">12</span>, [<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">2</span>, scope=<span class="string">'block0_2'</span>) <span class="comment"># size/2</span></span><br><span class="line"></span><br><span class="line">            net = _inverted_residual_block(net, <span class="number">6</span>, stride=<span class="number">1</span>, expansion=<span class="number">1</span>, scope=<span class="string">'block1_0'</span>)</span><br><span class="line"></span><br><span class="line">            dsn2 = net</span><br><span class="line">            net = _inverted_residual_block(net, <span class="number">12</span>, stride=<span class="number">2</span>, scope=<span class="string">'block2_0'</span>) <span class="comment"># size/4</span></span><br><span class="line">            net = _inverted_residual_block(net, <span class="number">12</span>, stride=<span class="number">1</span>, scope=<span class="string">'block2_1'</span>)</span><br><span class="line"></span><br><span class="line">            dsn3 = net</span><br><span class="line">            net = _inverted_residual_block(net, <span class="number">24</span>, stride=<span class="number">2</span>, scope=<span class="string">'block3_0'</span>) <span class="comment"># size/8</span></span><br><span class="line">            net = _inverted_residual_block(net, <span class="number">24</span>, stride=<span class="number">1</span>, scope=<span class="string">'block3_1'</span>) </span><br><span class="line">            net = _inverted_residual_block(net, <span class="number">24</span>, stride=<span class="number">1</span>, scope=<span class="string">'block3_2'</span>)</span><br><span class="line"></span><br><span class="line">            dsn4 = net</span><br><span class="line">            net = _inverted_residual_block(net, <span class="number">48</span>, stride=<span class="number">2</span>, scope=<span class="string">'block4_0'</span>) <span class="comment"># size/16</span></span><br><span class="line">            net = _inverted_residual_block(net, <span class="number">48</span>, stride=<span class="number">1</span>, scope=<span class="string">'block4_1'</span>) </span><br><span class="line">            net = _inverted_residual_block(net, <span class="number">48</span>, stride=<span class="number">1</span>, scope=<span class="string">'block4_2'</span>) </span><br><span class="line">            net = _inverted_residual_block(net, <span class="number">48</span>, stride=<span class="number">1</span>, scope=<span class="string">'block4_3'</span>) </span><br><span class="line"></span><br><span class="line">            net = _inverted_residual_block(net, <span class="number">64</span>, stride=<span class="number">1</span>, scope=<span class="string">'block5_0'</span>) </span><br><span class="line">            net = _inverted_residual_block(net, <span class="number">64</span>, stride=<span class="number">1</span>, scope=<span class="string">'block5_1'</span>)</span><br><span class="line">            net = _inverted_residual_block(net, <span class="number">64</span>, stride=<span class="number">1</span>, scope=<span class="string">'block5_2'</span>)</span><br><span class="line"></span><br><span class="line">            dsn5 = net</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">## dsn layers</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'dsn1'</span>):</span><br><span class="line">            dsn1 = _dsn_1x1_conv2d(dsn1, <span class="number">1</span>)</span><br><span class="line">            <span class="comment">## no need deconv2d</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'dsn2'</span>):</span><br><span class="line">            dsn2 = _dsn_1x1_conv2d(dsn2, <span class="number">1</span>)</span><br><span class="line">            dsn2 = _dsn_deconv2d_with_upsample_factor(dsn2, <span class="number">1</span>, upsample_factor = <span class="number">2</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'dsn3'</span>):</span><br><span class="line">            dsn3 = _dsn_1x1_conv2d(dsn3, <span class="number">1</span>)</span><br><span class="line">            dsn3 = _dsn_deconv2d_with_upsample_factor(dsn3, <span class="number">1</span>, upsample_factor = <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'dsn4'</span>):</span><br><span class="line">            dsn4 = _dsn_1x1_conv2d(dsn4, <span class="number">1</span>)</span><br><span class="line">            dsn4 = _dsn_deconv2d_with_upsample_factor(dsn4, <span class="number">1</span>, upsample_factor = <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'dsn5'</span>):</span><br><span class="line">            dsn5 = _dsn_1x1_conv2d(dsn5, <span class="number">1</span>)</span><br><span class="line">            dsn5 = _dsn_deconv2d_with_upsample_factor(dsn5, <span class="number">1</span>, upsample_factor = <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># dsn fuse</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'dsn_fuse'</span>):</span><br><span class="line">            dsn_fuse = tf.concat([dsn1, dsn2, dsn3, dsn4, dsn5], <span class="number">3</span>)</span><br><span class="line">            dsn_fuse = _output_1x1_conv2d(dsn_fuse, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dsn_fuse, dsn1, dsn2, dsn3, dsn4, dsn5</span><br></pre></td></tr></table></figure>
<p>这个 MobileNet V2 风格的 HED 网络，整体结构和 VGG 风格的 HED 并没有区别，只是把 VGG 里面用到的卷积层操作替换成了 MobileNet V2 对应的卷积层，另外，因为 MobileNet V2 的第一个卷积层就设置了 stride=2，并不匹配 dsn1 层的 size，所以额外添加了两个 stride=1 的普通卷积层，把它们的输出作为 dsn1 层。</p>
<h3 id="MobileNet-V2-As-Base-Net"><a href="#MobileNet-V2-As-Base-Net" class="headerlink" title="MobileNet V2 As Base Net"></a>MobileNet V2 As Base Net</h3><p>MobileNet 只是针对手机运行环境设计出来的执行 <em>分类任务</em> 的网络结构，但是，和同样执行分类任务的 ResNet、Inception、Xception 这一类网络结构类似，都可以作为执行其他任务的网络结构的 base net，提取输入 image 的 feature maps，我尝试过 mobilenet_v2_style_unet、mobilenet_v2_style_deeplab_v3plus、mobilenet_v2_style_ssd，都是可以看到效果的。</p>
<h3 id="Android-性能瓶颈"><a href="#Android-性能瓶颈" class="headerlink" title="Android 性能瓶颈"></a>Android 性能瓶颈</h3><p>作为一个参考值，在 iPhone 7 Plus 上运行这个 mobilenet_v2_style_hed 网络并且执行后续的找点算法，FPS 可以跑到12，基本满足实时性的需求。但是当尝试在 Android 上部署的时候，即便是在高价位高配置的机型上，FPS 也很低，卡顿现象很明显。</p>
<p>经过排查，找到了一些线索。在 iPhone 7 Plus 上，计算量的分布如下图所示：</p>
<div align="center"><br><img src="/images/mobilenet_v2_hed_node_summary_ios.png" alt="mobilenet_v2_hed_node_summary_ios"><br></div>

<p>红框中的三种操作占据了大部分的 CPU 时间，用这几个数值做一个粗略估算，1.0 / (32 + 30 + 10 + 6) = 12.8，这和检测到的 FPS 是比较吻合的，说明大量的计算时间都用在神经网络上了，OpenCV 实现的找点算法的耗时是很短的。</p>
<p>但是在 Android 上，情况则完全不一样了，如下图所示：</p>
<div align="center"><br><img src="/images/mobilenet_v2_hed_node_summary_android_32.png" alt="mobilenet_v2_hed_node_summary_android_32"><br></div>

<p>用红框里的数值计算一下，FPS = 1.0 / (232 + 76 + 29 + 16) = 2.8，达不到实时的要求。从上图还可以看出，在 Android 上，Batch Normalization 消耗了大量的计算时间，而且和 Conv2D 消耗的 CPU 时间相比，不在一个数量级上了，这就和 iOS 平台上完全不是同一种分布规律了。进一步 debug 后发现，我们 Android 平台的 app，由于一些历史原因被限定住了只能使用 <em>32bit</em> 的 .so 动态库，换成 <em>64bit</em> 的 TensorFlow 动态库在独立的 demo app 里面重新测量，mobilenet_v2_style_hed 在 Android 上的运行情况就和 iOS 的接近了，虽然还是比 iOS 慢，但是 CPU 耗时的统计数据是同一种分布规律了。</p>
<p>所以，性能瓶颈就在于 Batch Normalization 在 <em>32bit</em> 的 ARM CPU 环境中执行效率不高，尝试过使用一些编译器优化选项重新编译 <em>32bit</em> 的 TensorFlow 库，但是并没有明显的改善。最后的解决方案是退而求其次，使用 vgg_style_hed，并且不使用 Batch Normalization，经过这样的调整后，Android 上的统计数据如下图：</p>
<div align="center"><br><img src="/images/vgg_hed_node_summary_android_32.png" alt="vgg_hed_node_summary_android_32"><br></div>

<h3 id="关于-TensorFlow-Lite"><a href="#关于-TensorFlow-Lite" class="headerlink" title="关于 TensorFlow Lite"></a>关于 TensorFlow Lite</h3><p>在使用 TensorFlow 1.7 部署模型的时候，TensorFlow Lite 还未支持 transposed convolution，所以没有使用 TF Lite (目前 github 上已经看到有 Lite 版本的 <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/kernels/transpose_conv.cc" target="_blank" rel="noopener">transpose_conv.cc</a> 了)。TensorFlow Lite 目前发展的很快，以后在选择部署方案的时候，TensorFlow Lite 是优先于 TensorFlow Mobile 的。</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><h5 id="xavier-init"><a href="#xavier-init" class="headerlink" title="xavier init"></a>xavier init</h5><p><a href="https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow/36784797" target="_blank" rel="noopener">How to do Xavier initialization on TensorFlow
</a><br><a href="https://zhuanlan.zhihu.com/p/25110150" target="_blank" rel="noopener">聊一聊深度学习的weight initialization</a></p>
<h5 id="Batch-Normalization-1"><a href="#Batch-Normalization-1" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h5><p><a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html" target="_blank" rel="noopener">Understanding the backward pass through Batch Normalization Layer</a><br><a href="https://zhuanlan.zhihu.com/p/29974820" target="_blank" rel="noopener">机器学习里的黑色艺术：normalization, standardization, regularization</a><br><a href="https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow" target="_blank" rel="noopener">How could I use Batch Normalization in TensorFlow?
</a><br><a href="https://github.com/keras-team/keras/issues/5465" target="_blank" rel="noopener">add Batch Normalization immediately before non-linearity or after in Keras?</a>  </p>
<h5 id="1x1-Convolution"><a href="#1x1-Convolution" class="headerlink" title="1x1 Convolution"></a>1x1 Convolution</h5><p><a href="https://stats.stackexchange.com/questions/194142/what-does-1x1-convolution-mean-in-a-neural-network." target="_blank" rel="noopener">What does 1x1 convolution mean in a neural network?</a><br><a href="https://datascience.stackexchange.com/questions/12830/how-are-1x1-convolutions-the-same-as-a-fully-connected-layer" target="_blank" rel="noopener">How are 1x1 convolutions the same as a fully connected layer?</a><br><a href="https://iamaaditya.github.io/2016/03/one-by-one-convolution/" target="_blank" rel="noopener">One by One [ 1 x 1 ] Convolution - counter-intuitively useful</a>  </p>
<h5 id="Upsampling-amp-amp-Transposed-Convolution"><a href="#Upsampling-amp-amp-Transposed-Convolution" class="headerlink" title="Upsampling &amp;&amp; Transposed Convolution"></a>Upsampling &amp;&amp; Transposed Convolution</h5><p><a href="http://warmspringwinds.github.io/tensorflow/tf-slim/2016/11/22/upsampling-and-image-segmentation-with-tensorflow-and-tf-slim/" target="_blank" rel="noopener">Upsampling and Image Segmentation with Tensorflow and TF-Slim</a><br><a href="http://cv-tricks.com/image-segmentation/transpose-convolution-in-tensorflow/" target="_blank" rel="noopener">Image Segmentation using deconvolution layer in Tensorflow</a>  </p>
<h5 id="ResNet-amp-amp-Inception-amp-amp-Xception"><a href="#ResNet-amp-amp-Inception-amp-amp-Xception" class="headerlink" title="ResNet &amp;&amp; Inception &amp;&amp; Xception"></a>ResNet &amp;&amp; Inception &amp;&amp; Xception</h5><p><a href="http://teleported.in/posts/network-in-network/" target="_blank" rel="noopener">Network In Network architecture: The beginning of Inception</a><br><a href="https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32" target="_blank" rel="noopener">ResNets, HighwayNets, and DenseNets, Oh My!</a><br><a href="https://hacktilldawn.com/2016/09/25/inception-modules-explained-and-implemented/" target="_blank" rel="noopener">Inception modules: explained and implemented</a><br><a href="https://github.com/kwotsin/TensorFlow-Xception" target="_blank" rel="noopener">TensorFlow implementation of the Xception Model by François Chollet</a>  </p>
<h5 id="TensorFlow-Lite"><a href="#TensorFlow-Lite" class="headerlink" title="TensorFlow Lite"></a>TensorFlow Lite</h5><p><a href="http://developers.googleblog.cn/2018/06/tensorflow-lite-overview.html" target="_blank" rel="noopener">TensorFlow Lite 深度解析</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/TensorFlow/" rel="tag"># TensorFlow</a>
          
            <a href="/tags/CNN/" rel="tag"># CNN</a>
          
            <a href="/tags/卷积神经网络/" rel="tag"># 卷积神经网络</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/05/08/Document-Scanning-With-TensorFlow-And-OpenCV/" rel="next" title="手机端运行卷积神经网络的一次实践 -- 基于 TensorFlow 和 OpenCV 实现文档检测功能">
                <i class="fa fa-chevron-left"></i> 手机端运行卷积神经网络的一次实践 -- 基于 TensorFlow 和 OpenCV 实现文档检测功能
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div class="ds-thread" data-thread-key="2018/06/02/Document-Scanning-With-TensorFlow-And-OpenCV-Part-Two/"
           data-title="手机端运行卷积神经网络实现文档检测功能(二) -- 从 VGG 到 MobileNetV2 知识梳理" data-url="http://fengjian0106.github.io/2018/06/02/Document-Scanning-With-TensorFlow-And-OpenCV-Part-Two/">
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="FengJian" />
            
              <p class="site-author-name" itemprop="name">FengJian</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">22</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TensorFlow-Code-Style-For-CNN-Net"><span class="nav-number">2.</span> <span class="nav-text">TensorFlow Code Style For CNN Net</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#矩阵初始化"><span class="nav-number">3.</span> <span class="nav-text">矩阵初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Batch-Normalization"><span class="nav-number">4.</span> <span class="nav-text">Batch Normalization</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Batch-Normalization-的优势很明显，尽量使用"><span class="nav-number">4.0.1.</span> <span class="nav-text">Batch Normalization 的优势很明显，尽量使用</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#不需要使用-bias"><span class="nav-number">4.0.2.</span> <span class="nav-text">不需要使用 bias</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#在什么位置添加-Batch-Normalization"><span class="nav-number">4.0.3.</span> <span class="nav-text">在什么位置添加 Batch Normalization</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#是否还需要使用-Regularizer"><span class="nav-number">4.0.4.</span> <span class="nav-text">是否还需要使用 Regularizer</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#从卷积运算到各种卷积层"><span class="nav-number">5.</span> <span class="nav-text">从卷积运算到各种卷积层</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#卷积运算"><span class="nav-number">5.0.1.</span> <span class="nav-text">卷积运算</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#标准卷积层"><span class="nav-number">5.0.2.</span> <span class="nav-text">标准卷积层</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#One-By-One-卷积层"><span class="nav-number">5.0.3.</span> <span class="nav-text">One By One 卷积层</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Depthwise-卷积层"><span class="nav-number">5.0.4.</span> <span class="nav-text">Depthwise 卷积层</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Separable-卷积层"><span class="nav-number">5.0.5.</span> <span class="nav-text">Separable 卷积层</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dilated-Convolutions-Atrous-Convolution-扩张卷积-空洞卷积"><span class="nav-number">6.</span> <span class="nav-text">Dilated Convolutions / Atrous Convolution / 扩张卷积 / 空洞卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#转置卷积-反卷积的初始化"><span class="nav-number">7.</span> <span class="nav-text">转置卷积/反卷积的初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#从-VGG-到-ResNet、Inception、Xception"><span class="nav-number">8.</span> <span class="nav-text">从 VGG 到 ResNet、Inception、Xception</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#从-MobileNet-V1-到-MobileNet-V2"><span class="nav-number">9.</span> <span class="nav-text">从 MobileNet V1 到 MobileNet V2</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#MobileNet-V1"><span class="nav-number">9.0.1.</span> <span class="nav-text">MobileNet V1</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#MobileNet-V2"><span class="nav-number">9.0.2.</span> <span class="nav-text">MobileNet V2</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MobileNet-V2-Style-HED"><span class="nav-number">10.</span> <span class="nav-text">MobileNet V2 Style HED</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MobileNet-V2-As-Base-Net"><span class="nav-number">11.</span> <span class="nav-text">MobileNet V2 As Base Net</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Android-性能瓶颈"><span class="nav-number">12.</span> <span class="nav-text">Android 性能瓶颈</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#关于-TensorFlow-Lite"><span class="nav-number">13.</span> <span class="nav-text">关于 TensorFlow Lite</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参考资料"><span class="nav-number">14.</span> <span class="nav-text">参考资料</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#xavier-init"><span class="nav-number">14.0.1.</span> <span class="nav-text">xavier init</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Batch-Normalization-1"><span class="nav-number">14.0.2.</span> <span class="nav-text">Batch Normalization</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1x1-Convolution"><span class="nav-number">14.0.3.</span> <span class="nav-text">1x1 Convolution</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Upsampling-amp-amp-Transposed-Convolution"><span class="nav-number">14.0.4.</span> <span class="nav-text">Upsampling &amp;&amp; Transposed Convolution</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ResNet-amp-amp-Inception-amp-amp-Xception"><span class="nav-number">14.0.5.</span> <span class="nav-text">ResNet &amp;&amp; Inception &amp;&amp; Xception</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#TensorFlow-Lite"><span class="nav-number">14.0.6.</span> <span class="nav-text">TensorFlow Lite</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">FengJian</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"fengjian0106"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  


















  





  

  

  

  
  

  

  

  

</body>
</html>
